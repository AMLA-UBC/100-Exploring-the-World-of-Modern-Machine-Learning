{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AMLA-UBC/100-Exploring-the-World-of-Modern-Machine-Learning/blob/main/L1_L2_Regularization_Exercise.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap on Regularization\n",
        "\n",
        "Regularization helps prevent overfitting by adding a penalty term to the loss function during training.\n",
        "\n",
        "L1 and L2 regularization differ in the way they penalize the weights:\n",
        "- L2 penalizes $weight^2$\n",
        "- L1 penalizes $|weight|$.\n",
        "\n",
        "The derivative of L2 regularization is $2 * weight$, which serves as a force that decreases the weight by a certain percentage. However, this decrease is not enough to drive the weights to exactly 0, as even after billions of reductions, the weight will never reach zero.\n",
        "\n",
        "On the other hand, the derivative of L1 regularization is a constant, which serves as a force that subtracts a certain value from the weight. Thanks to the absolute values, the derivative of L1 has a discontinuity at 0, meaning that if the subtraction would have caused the weight to cross 0, it would be set to exactly 0.\n",
        "\n",
        "L1 regularization is more effective for wide models because it drives more weights to 0, resulting in fewer dimensions and lower memory usage. This is important because high-dimensional sparse vectors can result in huge models that require a lot of memory. L1 regularization encourages the weights to drop to exactly 0, where possible, saving RAM and reducing noise in the model.\n",
        "\n",
        "Wide models have a large number of input features, while deep models have many hidden layers."
      ],
      "metadata": {
        "id": "CqpTEGR-UvZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow"
      ],
      "metadata": {
        "id": "v-DonLDYYCWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBDOgXgAUYW3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "Y = raw_df.values[1::2, 2]\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activity_regularizer=tf.keras.regularizers.L2(1e-5)))\n",
        "\n",
        "# Compile the model with L1 regularization\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "mse1, _ = model.evaluate(X_test, y_test)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activity_regularizer=tf.keras.regularizers.l2(1e-5)))\n",
        "\n",
        "# Compile the model with L2 regularization\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'],)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "mse2, _ = model.evaluate(X_test, y_test)\n",
        "print(\"\\n\")\n",
        "print(\"Mean Squared Error with L1 Regularization:\", mse1)\n",
        "print(\"Mean Squared Error with L2 Regularization:\", mse2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice\n",
        "\n",
        "Let's revisit our solutions to the [Multiclass Classification Exercise](https://colab.research.google.com/github/AMLA-UBC/100-Exploring-the-World-of-Modern-Machine-Learning/blob/main/Multiclass_Classification_Exercise.ipynb) and retrain the model with regularizations. Note that adding too much regularization negative affects a CNN's ability to capture patterns in the training data and make accurate predictions. Does using regularization on CNNs improve the accuracy on test data?"
      ],
      "metadata": {
        "id": "fMdUifWBbR8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "# Use L1 regularization\n",
        "...\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "...\n",
        "\n",
        "\n",
        "# Train the model\n",
        "...\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "...\n",
        "\n",
        "\n",
        "# Build the model\n",
        "# Use L2 regularization\n",
        "...\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "...\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "...\n",
        "\n"
      ],
      "metadata": {
        "id": "2AwgJCAlbhxA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}