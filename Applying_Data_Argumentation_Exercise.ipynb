{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AMLA-UBC/100-Exploring-the-World-of-Modern-Machine-Learning/blob/main/Applying_Data_Argumentation_Exercise.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt_JCW8g6b6y"
      },
      "source": [
        "## Load and Preprocess [Fashion MNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist)\n",
        "\n",
        "Convolutional layers require the data to be a 4D array, with the dimensions representing the number of training examples, the number of channels, the image width, and the image height. That's why we reshape the data.\n",
        "\n",
        "First, let's see how well our CNN performs without data argumentation, using the test set as our validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "miKkVkM-1Vp2"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbfZCave5qIr"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Scale the data to the range [0, 1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Reshape the data to be used in a Conv2D layer\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Build a simple CNN\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=32,\n",
        "          epochs=3,\n",
        "          validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKKS0nCo5dCu"
      },
      "source": [
        "## ImageDataGenerator\n",
        "Applies various data augmentation techniques to help expand the size of the dataset and refine the model's robustness, thus mitigating the risk of overfitting.\n",
        "\n",
        "1. `rotation_range` controls the degree of random rotation that will be imposed on the images. It is set at 30, meaning the images will be spun by an angle that fluctuates between -30 and 30 degrees.\n",
        "\n",
        "2. `width_shift_range` and `height_shift_range` dictate the horizontal and vertical random shifts applied to the images, respectively. With a value of 0.2, the images will be shifted randomly by a fraction that ranges from -0.2 to 0.2 of their total width or height.\n",
        "\n",
        "3. `zoom_range` sets the extent of random zooming to be applied to the images. It is set at 0.2, meaning the images will undergo random zooming, either inward or outward, by a factor that lies between 0.8 and 1.2.\n",
        "\n",
        "4. `horizontal_flip` controls the random horizontal flipping of images. With a value of True, the images will have a 50% chance of being flipped horizontally.\n",
        "\n",
        "5. `fill_mode` defines the strategy for filling in any newly created pixels after image augmentation. The value is set to 'nearest', meaning any new pixels will be filled with the value of the closest existing pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgmjGfM-5nV4"
      },
      "outputs": [],
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Scale the data to the range [0, 1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Perform data augmentation\n",
        "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# Add the grayscale dimension\n",
        "x_train = x_train[..., np.newaxis]\n",
        "\n",
        "# Fit the data generator on the training data\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Build a simple CNN\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "          epochs=3,\n",
        "          validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szpSb6s3-Cd1"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Which method achieved higher accuracy? Why might that be the case? Hint: review our model architecture and dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXQ8HVGC-bmN"
      },
      "source": [
        "## Practice\n",
        "\n",
        "Let's use the STL10 dataset, which contains small images in 10 classes and 5000 images for each class, from `tensorflow_datasets`. Your goal for this exercise is to apply data argumentation to the STL10 dataset (fill in the ... places), as well as learn to research on your own and read documentations.\n",
        "\n",
        "An example of what you may want to google is \"randomly rotating images in tensorflow\". An example of what you may want to ask ChatGPT is \"list the TensorFlow 2 functions and explanations that allow me to apply data argumentation\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i4eVGTHZAVz3"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAtct5ik__UO"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Download the STL10 dataset, which may take a few minutes\n",
        "train = tfds.load(\"stl10\", split=\"train[:70%]\", as_supervised=True)\n",
        "test = tfds.load(\"stl10\", split=\"train[70%:]\", as_supervised=True)\n",
        "\n",
        "# Apply transformations to x and y, where x represents the training images and y represents the training labels\n",
        "train = train.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))\n",
        "train = train.map(lambda x, y: (..., y))\n",
        "train = train.map(lambda x, y: (..., y))\n",
        "train = train.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Use the pretrained ResNet50 model as our base\n",
        "resnet = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
        "model = tf.keras.Sequential([\n",
        "    resnet,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train,\n",
        "          epochs=1,\n",
        "          validation_data=test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bm9Illt4I4DO"
      },
      "source": [
        "## Further Analysis\n",
        "\n",
        "Using data argumentation in well-documented datasets such as Fashion MNIST is unnecessary. Additionally, our basic CNN model was too simple to learn to classify randomly argumented image data quickly.\n",
        "\n",
        "Data argumentation is only necessary when we have limited labelled data overall or for a particular class. This is because when the number of samples for each class in the training dataset is imbalanced, the model may not generalize well to new data. Data augmentation can help balance the under-represented classes by creating additional samples. [Google SafeSearch Mini V2](https://huggingface.co/FredZhang7/google-safesearch-mini-v2) shows an example of the effects of having class imbalance. However, it's worth noting that there are already open-source auto-labelling models availiable on HuggingFace to caption different types of images, videos, and audio. [BLIP](https://huggingface.co/docs/transformers/main/model_doc/blip) is an example.\n",
        "\n",
        "Data argumentation is also a powerful tool in object detection and segmentation tasks because the model needs to learn to detect objects in different orientations, scales, and positions. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
