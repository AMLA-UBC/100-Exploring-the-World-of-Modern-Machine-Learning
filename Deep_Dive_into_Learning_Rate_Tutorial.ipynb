{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AMLA-UBC/100-Exploring-the-World-of-Modern-Machine-Learning/blob/main/Deep_Dive_into_Learning_Rate_Tutorial.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install and Import the Required Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfvTxmyC8T9X"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIZ-bGug7kfy"
      },
      "source": [
        "# Load Tensorflow's Animals Dataset\n",
        "\n",
        "The dataset contains images of 10 different animals: cats, dogs, cows, horses, sheep, goats, ducks, chickens, rabbits, and pigs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8Yl9zgn7VSF"
      },
      "outputs": [],
      "source": [
        "# Load the animal dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.animals.load_data()\n",
        "\n",
        "# Reshape the data\n",
        "x_train = x_train.reshape(x_train.shape[0], 150, 150, 3)\n",
        "x_test = x_test.reshape(x_test.shape[0], 150, 150, 3)\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_EjxBRJ9A4g"
      },
      "source": [
        "# Change the Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWK-wSOn9IDT"
      },
      "source": [
        "Train the CNN with different learning rates. Play around with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dwL1gDx9DIj"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXgigl94861F"
      },
      "source": [
        "# Build a Convolutional Neural Netowrk (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrpCX2kz855B"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model = tf.keras.Sequential([\n",
        "    # Add a convolutional layer\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    # Add a pooling layer\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # Add a flatten layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # Add a dense layer\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    # Add a final output layer\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"10animals\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjybEv0Ba60t"
      },
      "source": [
        "# How to Find the Best Learning Rate?\n",
        "\n",
        "Learning rate is a critical hyperparameter in deep learning, which determines the step size that the optimizer takes when updating the model parameters. Selecting an appropriate learning rate can make a big difference in the training performance of your model. Here're a few ways to find the best learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dySlUeYir94e"
      },
      "source": [
        "One of the most straightforward methods to find an optimal learning rate is to **plot the learning rate vs. the loss**. You can start with a low learning rate, say 1e-7, and gradually increase it, for example, by multiplying it by 10 after every iteration. Plot the loss values for each learning rate, and look for the \"sweet spot\" where the loss starts to decrease and then suddenly increases. That is the learning rate you should use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wjnR4B-snd3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "learning_rates = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
        "loss_values = []\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(x_train, y_train, epochs=10)\n",
        "    loss_values.append(history.history['loss'][-1])\n",
        "\n",
        "plt.plot(learning_rates, loss_values)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGtoGF5YrEkO"
      },
      "source": [
        "Another method is to use a **learning rate schedule**, which starts with a high learning rate and gradually decreases it over time. A common schedule is the step decay schedule, where the learning rate is reduced by a factor of 0.1 after a certain number of epochs. You can also use other schedules like cyclical learning rate or cosine annealing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_tUTdVUrNBi"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "initial_learning_rate = 0.1\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
        "                                                             decay_steps=100000,\n",
        "                                                             decay_rate=0.96,\n",
        "                                                             staircase=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaH108C2rZT-"
      },
      "source": [
        "There are also **pre-written algorithms** that can help you find the best learning rate. For example, the learning rate finder and learning rate range test. They both plot the learning rate versus the loss, but with some added functionality, such as automatically increasing the learning rate after each iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmF7vYlNrgVD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the learning rate range\n",
        "lr_start = 1e-7\n",
        "lr_end = 1\n",
        "num_steps = 200\n",
        "\n",
        "# Create a learning rate schedule\n",
        "learning_rates = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    lr_start,\n",
        "    decay_steps=num_steps,\n",
        "    decay_rate=(lr_end/lr_start)**(1/num_steps),\n",
        "    staircase=True)\n",
        "\n",
        "# Store the loss values\n",
        "losses = []\n",
        "\n",
        "# Loop over the learning rates\n",
        "for i, learning_rate in enumerate(learning_rates):\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model for a single step\n",
        "    model.fit(x_train[:100], y_train[:100], epochs=1, batch_size=32, verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, _ = model.evaluate(x_test, y_test, verbose=0)\n",
        "    losses.append(loss)\n",
        "\n",
        "# Plot the learning rate versus the loss\n",
        "plt.plot(learning_rates, losses)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning rate')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Choose the learning rate with the lowest loss\n",
        "best_learning_rate = learning_rates[np.argmin(losses)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5zIc5x5tSXN"
      },
      "source": [
        "Once you've found a good learning rate, **validate your model** with a test or validation set. It's important to keep in mind that the optimal learning rate may change based on the architecture and dataset you're using. So, experiment and validate with different learning rates to ensure that you've found the best one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5pOtQWutc2N"
      },
      "outputs": [],
      "source": [
        "# Define a list of learning rates to experiment with\n",
        "learning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1]\n",
        "\n",
        "# Initialize a list to store the test accuracy for each learning rate\n",
        "accuracies = []\n",
        "\n",
        "# Loop through each learning rate\n",
        "for learning_rate in learning_rates:\n",
        "    # Compile the model with the current learning rate\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "    print('Test accuracy for learning rate', learning_rate, ':', test_acc)\n",
        "    accuracies.append(test_acc)\n",
        "\n",
        "# Find the learning rate with the highest test accuracy\n",
        "best_learning_rate = learning_rates[accuracies.index(max(accuracies))]\n",
        "print('Best learning rate:', best_learning_rate)\n",
        "\n",
        "# Compile the model with the best learning rate\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=best_learning_rate),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with the best learning rate\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "# Evaluate the model with the best learning rate\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy with best learning rate:', test_acc)\n",
        "\n",
        "# Save the model with the best learning rate\n",
        "model.save(\"10animals_best_lr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8wVruVSul2K"
      },
      "source": [
        "In conclusion, finding the best learning rate may require a combination of trial and error, experimentation, and validation. The above methods should give you a good starting point, and be prepared to tweak and fine-tune the learning rate based on your specific use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqxGc9tVvFpV"
      },
      "source": [
        "# Popular Image Classification Datasets on Kaggle\n",
        "\n",
        "The Kaggle image classification datasets are a treasure trove of visual data for all to explore. From the complex patterns of Deepfake images to the simple strokes of the QuickDraw dataset, these collections offer a fascinating glimpse into the ever-evolving world of AI. For those looking to build image classification models for practice, here are some of the most popular image classification datasets available on Kaggle:\n",
        "\n",
        "- Doodles Dataset: `A collection of cartoon drawings from the Google QuickDraw project, offering an array of categories from which to classify images.`\n",
        "\n",
        "- Chest X-Ray Images: `A dataset of over 100,000 chest X-ray images from 15 different classes, with labels that identify the area of abnormality.`\n",
        "\n",
        "- Fruits 360 Dataset: `A dataset of images of over 120 different types of fruits, classified by size, shape, and color.`\n",
        "\n",
        "- CelebFaces Attributes Dataset (CelebA): `A dataset of over 200,000 celebrity faces, categorized by age, gender, and ethnicity.`\n",
        "\n",
        "- Deepfake Detection Challenge: `A dataset of millions of images and videos of deepfakes, generated using artificial intelligence algorithms.`\n",
        "\n",
        "- Plant Seedlings Dataset: `A dataset of images of over 1,500 different species of plant seedlings, classified by species.`\n",
        "\n",
        "- Traffic Sign Recognition: `A dataset of over 40,000 images of traffic signs, categorized by type of sign.`"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
