{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AMLA-UBC/100-Exploring-the-World-of-Modern-Machine-Learning/blob/main/Modern_CNN_on_Image_Classification_Tutorial.ipynb)"
      ],
      "metadata": {
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and load the required modules"
      ],
      "metadata": {
        "id": "vffbEHDz9XW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow tensorflow_datasets\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "PRyEzVRk9QBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the train and test sets"
      ],
      "metadata": {
        "id": "rWepQr-N-I_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "dataset, info = tfds.load('horses_or_humans', with_info=True, as_supervised=True)\n",
        "\n",
        "# Splitting the dataset\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "# Preprocessing the data\n",
        "def preprocess(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = image/255.0\n",
        "    image = tf.image.resize(image, (299, 299))\n",
        "    return image, label\n",
        "\n",
        "# Applying the preprocess function to each item in the dataset\n",
        "train_dataset = train_dataset.map(preprocess).batch(32)\n",
        "test_dataset = test_dataset.map(preprocess).batch(32)"
      ],
      "metadata": {
        "id": "Urqo2S2695Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the train model function"
      ],
      "metadata": {
        "id": "aAKRYdjt-vNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model):\n",
        "  # Compiling the model\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # Training the model\n",
        "  model.fit(train_dataset, epochs=10)\n",
        "\n",
        "  # Evaluating the model\n",
        "  test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "\n",
        "  print('Test Loss: {}, Test Accuracy: {}'.format(test_loss, test_accuracy))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "DBEe0pTo-nmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imagenet Dataset\n",
        "- Moving forward, you will see or hear about the Imagenet Dataset. Imagenet is an incredibly popular dataset used in the field of machine learning. It is a large collection of over 14 million labeled images, organized into over 20,000 categories. It is a great resource for training and testing computer vision algorithms, which are used to identify objects in photographs.\n",
        "\n",
        "- Many machine learning models use Imagenet to train a pretrained version. This allows the model to become more accurate and better at recognizing objects in images. The dataset can be downloaded from the Imagenet website: http://image-net.org/download-images"
      ],
      "metadata": {
        "id": "0Uv7V1sb_so1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolution Operation\n",
        "- A convolution operation is a mathematical operation that is used to extract features from an image.\n",
        "\n",
        "\n",
        "- It works by sliding a small matrix (called a “kernel”) over the image and computing the dot product between the kernel and the original image. This operation is repeated for each pixel in the image, and the output of the convolution is a feature map. This feature map can then be used to detect objects or patterns in the image."
      ],
      "metadata": {
        "id": "P2CJ5AMpDm3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xception\n",
        "- The Xception architecture is a deep convolutional neural network (CNN) that was introduced in 2016 by Google. It is a powerful model that has been proven to perform well in image classification tasks.\n",
        "\n",
        "\n",
        "\n",
        "- Compared to other models such as MobileNetV2 and DenseNet121, Xception is unique because it uses a Depthwise Separable Convolution, which is a type of convolutional layer that is more efficient than regular convolutional layers.\n",
        "\n",
        "\n",
        "\n",
        "- Depthwise Separable convolution is a type of convolutional layer that reduces the number of parameters and computations required in a CNN model. It works by first applying a depthwise convolution, which splits the input channels into separate “groups” and then applies a convolution operation on each of these groups. This reduces the number of parameters and computations required compared to a regular convolutional layer."
      ],
      "metadata": {
        "id": "h6aUQSDwcDHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Xception block\n",
        "inputs = tf.keras.layers.Input(shape=(299, 299, 3))\n",
        "x = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(inputs)\n",
        "x = tf.keras.layers.Activation('relu')(x)\n",
        "x = tf.keras.layers.SeparableConv2D(128, (3, 3), padding='same')(x)\n",
        "x = tf.keras.layers.Activation('relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
        "tf.keras.Model(inputs, x, name=\"Xception\").summary()"
      ],
      "metadata": {
        "id": "3lJgB43OBtX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the pretrained Xception model\n",
        "model = tf.keras.applications.Xception(weights='imagenet', include_top=True, input_shape=(299, 299, 3))\n",
        "\n",
        "# Train the model\n",
        "model = train_model(model)\n",
        "\n",
        "# Save the model\n",
        "model.save('horses_or_humans_xception.h5')"
      ],
      "metadata": {
        "id": "n861vWMP9uzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# InceptionResNetV2"
      ],
      "metadata": {
        "id": "eEOXbl6teocZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic InceptionResNetV2 block\n",
        "inputs = tf.keras.layers.Input(shape=(299, 299, 3))\n",
        "x = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(inputs)\n",
        "x = tf.keras.layers.Activation('relu')(x)\n",
        "x = tf.keras.layers.Conv2D(128, (3, 3), padding='same')(x)\n",
        "x = tf.keras.layers.Activation('relu')(x)\n",
        "x = tf.keras.layers.Conv2D(128, (3, 3), padding='same')(x)\n",
        "x = tf.keras.layers.Activation('relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
        "tf.keras.Model(inputs, x, name=\"InceptionResNetV2\").summary()"
      ],
      "metadata": {
        "id": "vRHljLU3fDTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the pretrained InceptionResNetV2 model\n",
        "model = tf.keras.applications.InceptionResNetV2(weights='imagenet', include_top=True, input_shape=(299, 299, 3))\n",
        "\n",
        "# Train the model\n",
        "model = train_model(model)\n",
        "\n",
        "# Save the model\n",
        "model.save('horses_or_humans_inceptionresnetv2.h5')"
      ],
      "metadata": {
        "id": "lggTkf3KWbKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# MobileNetV2\n",
        "- Designed to be lightweight and efficient for use on mobile devices\n",
        "- Smaller number of parameters than other models\n",
        "- May not perform as well on tasks that require a higher level of abstraction and feature extraction\n",
        "- May not be able to handle large datasets as effectively as other models\n"
      ],
      "metadata": {
        "id": "kXME8-I_eqk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic MobileNetV2 block \n",
        "inputs = tf.keras.layers.Input(shape=(299, 299, 3))\n",
        "x = tf.keras.layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inputs)\n",
        "x = tf.keras.layers.ReLU()(x)\n",
        "x = tf.keras.layers.ReLU()(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "tf.keras.Model(inputs, x, name=\"MobileNetV2\").summary()"
      ],
      "metadata": {
        "id": "pN1U5eQmgFHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the pretrained MobileNetV2 model\n",
        "model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=True, input_shape=(299, 299, 3))\n",
        "\n",
        "# Train the model\n",
        "model = train_model(model)\n",
        "\n",
        "# Save the model\n",
        "model.save('horses_or_humans_mobilenetv2.h5')"
      ],
      "metadata": {
        "id": "sSBNQuhSXA3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# DenseNet121\n",
        "\n"
      ],
      "metadata": {
        "id": "wWLgcUSVesH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic DenseNet121 block \n",
        "inputs = tf.keras.layers.Input(shape=(299, 299, 3))\n",
        "x = tf.keras.layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same')(inputs)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.ReLU()(x)\n",
        "x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "tf.keras.Model(inputs, x, name=\"DenseNet121\").summary()"
      ],
      "metadata": {
        "id": "3BJuXBDTrvTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the pretrained DenseNet121 model\n",
        "model = tf.keras.applications.DenseNet121(weights='imagenet', include_top=True, input_shape=(299, 299, 3))\n",
        "\n",
        "# Train the model\n",
        "model = train_model(model)\n",
        "\n",
        "# Save the model\n",
        "model.save('horses_or_humans_densenet121.h5')"
      ],
      "metadata": {
        "id": "ASHEhBBhXfNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ResNet50\n",
        "- ResNet50 is a powerful image classification model that stands out from other models due to its unique architecture. It was developed by Microsoft Research and uses a technique called Deep Residual Learning, which helps it to learn more effectively from data and achieve better performance.\n",
        "\n",
        "- The main idea behind ResNet50 is that it helps to reduce the training error of a deep neural network by introducing a “shortcut” or “skip connection” between layers. This shortcut allows the model to learn more quickly and accurately by allowing information to flow directly from one layer to the next, instead of having to go through all the layers in between.\n",
        "\n",
        "- This makes ResNet50 a great choice for image classification tasks, as it can quickly learn complex patterns and features from the data, leading to more accurate predictions. Additionally, ResNet50 is a very efficient model and requires less data and computing power than other models."
      ],
      "metadata": {
        "id": "zPS9Lx0oetcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic ResNet50 block \n",
        "inputs = tf.keras.layers.Input(shape=(299, 299, 3))\n",
        "x = tf.keras.layers.Conv2D(64, (7, 7), strides=(2, 2), padding='same')(inputs)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.ReLU()(x)\n",
        "x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "tf.keras.Model(inputs, x, name=\"ResNet50\").summary()"
      ],
      "metadata": {
        "id": "iaJdbY8Rsifl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the pretrained ResNet50 model\n",
        "model = tf.keras.applications.ResNet50(weights='imagenet', include_top=True, input_shape=(299, 299, 3))\n",
        "\n",
        "# Train the model\n",
        "model = train_model(model)\n",
        "\n",
        "# Save the model\n",
        "model.save('horses_or_humans_resnet50.h5')"
      ],
      "metadata": {
        "id": "H3a7WZUVX6CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Overall, the models that scores the highest accuracy on large multi-class datasets are DenseNet121, InceptionResNetV2, and ResNet50. However, the most efficient model is MobileNetV2 because of its low disk space and RAM usage. The model that is best suited for a specific task depends on the specific requirements of that task, such as the size of the dataset, the amount of memory and computational resources available, and the desired level of abstraction and feature extraction."
      ],
      "metadata": {
        "id": "6_2MTgdlevAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Where can I find datasets outside of Kaggle?\n",
        "\n",
        "1. Google Cloud Datasets - https://cloud.google.com/ai-platform/data/docs/datasets\n",
        "2. TensorFlow Datasets - https://www.tensorflow.org/datasets\n",
        "3. Open Images - https://storage.googleapis.com/openimages/web/index.html\n",
        "4. ImageNet - http://www.image-net.org/\n",
        "5. MNIST Database - http://yann.lecun.com/exdb/mnist/\n",
        "6. Fashion MNIST - https://github.com/zalandoresearch/fashion-mnist\n",
        "7. CIFAR-10 - https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "8. UCI Machine Learning Repository - https://archive.ics.uci.edu/ml/index.php\n",
        "9. Google Trends Data - https://trends.google.com/trends/"
      ],
      "metadata": {
        "id": "2FVUVMEtGo47"
      }
    }
  ]
}
